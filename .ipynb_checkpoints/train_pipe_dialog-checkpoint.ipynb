{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5434c54-063d-42ca-b55e-770748c56bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "Cloning into 'instruct_rugptlarge'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "remote: Total 89 (delta 42), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (89/89), 1.43 MiB | 3.29 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 4.43 GiB | 86.11 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/AlexWortega/instruct_rugptlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453186ed-5c64-497b-ad55-b33426939cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting dask\n",
      "  Downloading dask-2023.4.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 9.5 MB/s eta 0:00:01\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "\u001b[?25hCollecting partd>=1.2.0\n",
      "  Downloading partd-1.4.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (2022.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (6.0.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (2.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/alexw/.local/lib/python3.8/site-packages (from dask) (8.1.3)\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alexw/.local/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask) (3.15.0)\n",
      "Installing collected packages: locket, partd, dask\n",
      "Successfully installed dask-2023.4.0 locket-1.0.0 partd-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e64c48-160f-4f76-a970-72a1e763a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a386dbb0-ac53-4425-8b26-13bdada89ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 27 17:29:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    40W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    69W / 300W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d59ac5-d63b-4a35-947d-8ebdaaf4bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9abd70-d560-4033-915f-688c6515d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68aac0b0-5db8-4324-9ab7-9f5797ad83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load(\"config.yaml\") #get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed32691-fd69-4c46-acf7-507a3520bba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 1536)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(conf.model.model_name_or_path)\n",
    "special_tokens_dict = {'additional_special_tokens': ['user:', 'bot:', '<next>']}\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model = GPT2LMHeadModel.from_pretrained(conf.model.model_name_or_path)\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed97099-b504-4338-b42d-5b7540b4a32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a869143-0756-42d5-8e2e-ccb063980e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4164cd94d1047b81\n",
      "Found cached dataset ru_turbo_saiga (/home/alexw/.cache/huggingface/datasets/IlyaGusev___ru_turbo_saiga/default-4164cd94d1047b81/0.0.1/28415c7ce4ae7d7de9064f6f7e18a089ba79edce3adc4a43fdbb09e9cd39f16c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830bb4b1ce749e3a34e482f30cd6edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(conf.args.dataset_name, data_dir='./IlyaGusev___ru_turbo_saiga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b942c1-d413-4b65-93b6-f5769ec6bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.tokenized = []\n",
    "        al = dataset['train']['messages']\n",
    "        for dia in tqdm(al):\n",
    "            dialog = \"\"\n",
    "            for sen in zip(dia['role'], dia['content']):\n",
    "                dialog += ': '.join(sen) + ' <next> '\n",
    "            enc = self._encode(text=sen, tokenizer=tokenizer)\n",
    "            self.tokenized += [enc]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.tokenized[item] \n",
    "\n",
    "    @staticmethod\n",
    "    def _encode(text, tokenizer):\n",
    "        encoded_sample = tokenizer.encode(text, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "        return encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b739053b-3226-4862-a299-b1a3effadb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36352/36352 [00:22<00:00, 1591.60it/s]\n"
     ]
    }
   ],
   "source": [
    "data = DialogDataset(tokenizer=tokenizer, dataset=dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        data, shuffle=True, batch_size=conf.args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd798fa-c5d3-4a89-8bd3-4b4c26815fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'saiga_train.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f94564d-4c5d-4c6f-8903-27fd18ef1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find saiga_train.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarina_romanova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/opt/alexw/Kar/saiga/wandb/run-20230427_172953-opy3tb9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karina_romanova/Saiga_dialog_train/runs/opy3tb9l' target=\"_blank\">ethereal-pine-6</a></strong> to <a href='https://wandb.ai/karina_romanova/Saiga_dialog_train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karina_romanova/Saiga_dialog_train' target=\"_blank\">https://wandb.ai/karina_romanova/Saiga_dialog_train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karina_romanova/Saiga_dialog_train/runs/opy3tb9l' target=\"_blank\">https://wandb.ai/karina_romanova/Saiga_dialog_train/runs/opy3tb9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/karina_romanova/Saiga_dialog_train/runs/opy3tb9l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc2df39b7f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Saiga_dialog_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1af581b6-423f-4556-ae9e-edca26563248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, decay):\n",
    "        super(EMA, self).__init__()\n",
    "        self.decay = decay\n",
    "        self.shadow_params = {}\n",
    "\n",
    "    def forward(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if name not in self.shadow_params:\n",
    "                    self.shadow_params[name] = param.data.clone()\n",
    "                else:\n",
    "                    self.shadow_params[name] -= (1 - self.decay) * (self.shadow_params[name] - param.data)\n",
    "                param.data = self.shadow_params[name]\n",
    "                \n",
    "ema = EMA(decay=0.992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d24b9-3653-47a6-b537-6f109f28d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexw/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  1%|▏         | 104/7271 [02:06<2:24:56,  1.21s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "ema = EMA(decay=0.992)\n",
    "optimizer = AdamW(model.parameters(), lr = conf.args.sft_lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader)\n",
    "            )\n",
    "model.train()\n",
    "for batch in tqdm(train_dataloader):\n",
    "    \n",
    "    t = batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_ids = t, labels=t)[0]\n",
    "\n",
    "    wandb.log({\"loss\":  loss})\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    ema(model)\n",
    "    scheduler.step()\n",
    "model.eval()\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad40692a-7380-4ba0-8808-673d8445f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('checkpoint_alldia_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2b658a4-be03-4abb-b5e2-09b7a334c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " прив\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m inp_prepared \u001b[38;5;241m=\u001b[39m context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<next>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m inp \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<next>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m prepared \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(inp_prepared, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m generated \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1242\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# decoder-only models should use left-padding for generation\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1241\u001b[0m         generation_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1242\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43minputs_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mpad_token_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1243\u001b[0m     ):\n\u001b[1;32m   1244\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1247\u001b[0m         )\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "        \"min_length\": 20,\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.7,\n",
    "        \"do_sample\": True,  \n",
    "        \"early_stopping\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"use_cache\": True,\n",
    "        \"repetition_penalty\": 1.5,  \n",
    "        \"length_penalty\": 1.2,  \n",
    "        \"num_beams\": 4,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "context = ''\n",
    "inp = ''\n",
    "while inp != 'stop':\n",
    "    inp = input()\n",
    "    inp_prepared = context + '<next>' + 'user: ' + inp + '<next>'\n",
    "    prepared = tokenizer.encode(inp_prepared, return_tensors='pt')[0].to(device)\n",
    "    out = model.generate(prepared, **gen_kwargs)\n",
    "    generated = tokenizer.decode(out, skip_special_tokens=True)\n",
    "    print(generated)\n",
    "    context += inp_prepared\n",
    "    context += 'bot: ' + generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31e9a9-a12e-4daa-9950-2a83309809f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_seqs(q,model, k=2):\n",
    "    gen_kwargs = {\n",
    "        \"min_length\": 20,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.7,\n",
    "        \"do_sample\": True,  \n",
    "        \"early_stopping\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"use_cache\": True,\n",
    "        \"repetition_penalty\": 1.5,  \n",
    "        \"length_penalty\": 1.2,  \n",
    "        \"num_beams\": 4,\n",
    "        \"num_return_sequences\": k\n",
    "    }\n",
    "    q = q + '<instructionS>'\n",
    "    t = tokenizer.encode(q, return_tensors='pt').to(device)\n",
    "    g = model.generate(t, **gen_kwargs)\n",
    "    generated_sequences = tokenizer.batch_decode(g, skip_special_tokens=True)\n",
    "    \n",
    "    return  generated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cce4a5-818c-4ddf-9e6b-d34a859e7fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
